{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6761d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6308235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_induction_dataset(n, k=4, seed=None):\n",
    "    \"\"\"\n",
    "    GÃ©nÃ¨re des sÃ©quences de la forme [A, 1, B, 2, A, ?] oÃ¹ le modÃ¨le doit prÃ©dire '1'.\n",
    "    \n",
    "    Args:\n",
    "        n: nombre de sÃ©quences\n",
    "        k: nombre de clÃ©s diffÃ©rentes (A, B, ...)\n",
    "        seed: pour reproductibilitÃ©\n",
    "\n",
    "    Returns:\n",
    "        X: [n, T, D] - embeddings one-hot\n",
    "        y: [n] - target class (indice de la valeur Ã  prÃ©dire)\n",
    "        char_to_id, id_to_char: dictionnaires utiles\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # DÃ©finir l'alphabet : A, B, ..., + valeurs : 1, 2, ...\n",
    "    letters = [chr(ord('A') + i) for i in range(k)]\n",
    "    values = [str(i + 1) for i in range(k)]\n",
    "    special_token = '?'  # Ã  complÃ©ter\n",
    "\n",
    "    tokens = letters + values + [special_token]\n",
    "    char_to_id = {c: i for i, c in enumerate(tokens)}\n",
    "    id_to_char = {i: c for c, i in char_to_id.items()}\n",
    "    D = len(tokens)\n",
    "    eye = np.eye(D)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        keys = random.sample(letters, k)\n",
    "        random.shuffle(values)\n",
    "        mapping = list(zip(keys, values))  # e.g. A->3, B->1\n",
    "\n",
    "        # On choisit une des paires Ã  rÃ©pÃ©ter\n",
    "        target_key, target_value = random.choice(mapping)\n",
    "\n",
    "        # SÃ©quence = [A, 3, B, 1, ...] + [A, ?]\n",
    "        sequence = []\n",
    "        for k_, v_ in mapping:\n",
    "            sequence.extend([k_, v_])\n",
    "        sequence.extend([target_key, special_token])\n",
    "\n",
    "        x_seq = [eye[char_to_id[tok]] for tok in sequence]\n",
    "        X.append(x_seq)\n",
    "        y.append(char_to_id[target_value])  # prÃ©dire la valeur (classe)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(X, dtype=torch.float32),  # [n, T, D]\n",
    "        torch.tensor(y, dtype=torch.long),     # [n]\n",
    "        char_to_id,\n",
    "        id_to_char\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d762f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_induction_sequences(X, y, id_to_char):\n",
    "    \"\"\"\n",
    "    Affiche les sÃ©quences d'induction et la cible Ã  prÃ©dire.\n",
    "    \n",
    "    Args:\n",
    "        X: [n, T, D] - embeddings one-hot\n",
    "        y: [n] - target ids (valeurs Ã  prÃ©dire)\n",
    "        id_to_char: dictionnaire id -> caractÃ¨re\n",
    "    \"\"\"\n",
    "    for i in range(len(X)):\n",
    "        seq_tensor = X[i]  # [T, D]\n",
    "        seq_ids = seq_tensor.argmax(dim=-1).tolist()\n",
    "        seq_chars = [id_to_char[idx] for idx in seq_ids]\n",
    "        target_char = id_to_char[y[i].item()]\n",
    "        print(f\"ðŸ“œ Sequence {i+1}: {' '.join(seq_chars)}\")\n",
    "        print(f\"ðŸŽ¯ Target: {target_char}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcf244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class AttentionOnlyBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.attn_weights = None  # pour debug/visualisation\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(Q.size(-1))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        self.attn_weights = attn_weights.detach().cpu()  # pour debug\n",
    "        out = torch.matmul(attn_weights, V)\n",
    "        return self.norm(x + out)\n",
    "\n",
    "class InductionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, num_layers=2, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.blocks = nn.ModuleList([AttentionOnlyBlock(d_model) for _ in range(num_layers)])\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, D]\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.classifier(x)  # [B, T, C] : logits par position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5312b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "T = 2*D+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252272ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class AttentionOnlyBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(Q.size(-1))\n",
    "        weights = F.softmax(attn, dim=-1)\n",
    "        self.attn_weights = weights.detach().cpu()\n",
    "        x = x + torch.matmul(weights, V)\n",
    "        return self.norm(x)\n",
    "\n",
    "class AttentionOnlyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, num_layers=2, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.layers = nn.ModuleList([AttentionOnlyBlock(d_model) for _ in range(num_layers)])\n",
    "        self.output = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]  # On ne garde que le dernier token\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81954e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                predicted = torch.argmax(preds, dim=1)\n",
    "                correct += (predicted == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch}/{epochs} | Train Loss: {total_loss/len(train_loader):.4f} | Val Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c772bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# HyperparamÃ¨tres\n",
    "T = 10\n",
    "D = 5\n",
    "n = 30000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99745e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, char_to_id, id_to_char = generate_induction_dataset(n, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aca6f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '5': 9,\n",
       " '?': 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda9242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,  y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c382cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 1.6293 | Val Acc: 0.2027\n",
      "Epoch 2/20 | Train Loss: 1.6176 | Val Acc: 0.2028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m input_dim  \u001b[38;5;66;03m# car lettres + chiffres sont one-hot encodÃ©s dans une seule dimension\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AttentionOnlyClassifier(input_dim\u001b[38;5;241m=\u001b[39minput_dim, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, epochs, lr, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(preds, yb)\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mAttentionOnlyClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# On ne garde que le dernier token\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mAttentionOnlyBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(weights, V)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28minput\u001b[39m, normalized_shape, weight, bias, eps, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/backends/__init__.py:32\u001b[0m, in \u001b[0;36mContextProp.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, objtype):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[-1]\n",
    "num_classes = input_dim  # car lettres + chiffres sont one-hot encodÃ©s dans une seule dimension\n",
    "model = AttentionOnlyClassifier(input_dim=input_dim, num_classes=num_classes, d_model=128, num_layers=2)\n",
    "\n",
    "train_model(model, train_loader, test_loader, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5340eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, data_loader, id_to_char, device='cuda' if torch.cuda.is_available() else 'cpu', num_batches=1):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    vocab_size = len(id_to_char)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (xb, yb) in enumerate(data_loader):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)  # [B, num_classes]\n",
    "            preds = torch.argmax(logits, dim=-1)  # [B]\n",
    "\n",
    "            for j in range(xb.size(0)):\n",
    "                sequence_ids = torch.argmax(xb[j][:, :vocab_size], dim=-1).cpu().numpy()  # [T]\n",
    "                sequence = [id_to_char[i] for i in sequence_ids]\n",
    "\n",
    "                print(f\"ðŸ”¡ Input sequence  : {''.join(sequence)}\")\n",
    "                print(f\"ðŸŽ¯ Target letter   : {id_to_char[yb[j].item()]}\")\n",
    "                print(f\"ðŸ”® Predicted letter: {id_to_char[preds[j].item()]}\")\n",
    "                print(\"-\" * 40)\n",
    "\n",
    "            if i + 1 >= num_batches:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dd89ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¡ Input sequence  : C1E3B2A4D5B?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A1E4D5B2C3D?\n",
      "ðŸŽ¯ Target letter   : 5\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D4B2A5C3E1D?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A4D5B3C2E1C?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B4A3E1D5C2A?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E3C2D4B1A5E?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A4D2E1B5C3C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E5D2B1A3C4C?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C3A5E2B1D4B?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E5D2A4C3B1C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B5E4C1D2A3C?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D2C3A4B5E1B?\n",
      "ðŸŽ¯ Target letter   : 5\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A2E1D3C4B5A?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D4B5C2A1E3D?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E4B5C1A2D3C?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D1C3B4A2E5A?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D5C2B3E4A1D?\n",
      "ðŸŽ¯ Target letter   : 5\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E2B4A3C1D5B?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C4A1B5D2E3A?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C5E1B4A3D2D?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C3B5E1A2D4C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B2E5C4D1A3B?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B5A3C1E4D2B?\n",
      "ðŸŽ¯ Target letter   : 5\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E1C3A4D2B5C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A2C1D4E3B5C?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B3C1A5E2D4D?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E4D2A3C1B5A?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D2C3B5E1A4A?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D2A1C5E3B4D?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C1D4A3B5E2E?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C2B1A5E3D4E?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A5B3E1D4C2E?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B2E5D1A4C3D?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D2E4B5C3A1C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C1A2E3D4B5E?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C3B1D2A4E5D?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A4C1B2D3E5C?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D1C5B2A4E3D?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D3A1E2B4C5D?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C5D4A3B2E1E?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A1C4E3D2B5A?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B4D2A5C3E1C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C2E5D1A3B4B?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : E5C4A2B1D3B?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A3C1B2D5E4D?\n",
      "ðŸŽ¯ Target letter   : 5\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C3E2A1B5D4C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A5D1C3B2E4C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A3C5D2E1B4D?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C4A1B3E5D2B?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D4B5C2E1A3A?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B1A4E3C5D2A?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A1B4C5E3D2A?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B1C2E5A4D3E?\n",
      "ðŸŽ¯ Target letter   : 5\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B1A3D2E4C5D?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D4A3C1E5B2B?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C5B4E2D1A3E?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D2B3A1E4C5A?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B2A1D5C3E4C?\n",
      "ðŸŽ¯ Target letter   : 3\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : A2E3C1D4B5C?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : B3E4C5A2D1A?\n",
      "ðŸŽ¯ Target letter   : 2\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D3E4C2B1A5A?\n",
      "ðŸŽ¯ Target letter   : 5\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : C3A5B1D4E2D?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D1A2E3B5C4D?\n",
      "ðŸŽ¯ Target letter   : 1\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n",
      "ðŸ”¡ Input sequence  : D5B1A3C2E4E?\n",
      "ðŸŽ¯ Target letter   : 4\n",
      "ðŸ”® Predicted letter: 2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "show_predictions(model, test_loader, id_to_char, num_batches=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331384f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_char"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
