{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(N, D, M, P, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Poids du professeur : 2 blocs (query/key)\n",
    "    W_q = torch.randn(P, D)\n",
    "    W_k = torch.randn(P, D)\n",
    "\n",
    "    x = torch.randn(N, D, M)  # N séquences d’entrée\n",
    "\n",
    "    y = torch.zeros(N, M, D)\n",
    "    for i in range(N):\n",
    "        x_i = x[i]  # [D, M]\n",
    "\n",
    "        # Projections query/key : [P, M]\n",
    "        Zq = W_q @ x_i / D**0.5\n",
    "        Zk = W_k @ x_i / D**0.5\n",
    "\n",
    "        # Attention : [M, M]\n",
    "        A = torch.softmax(Zq.T @ Zk, dim=-1)\n",
    "\n",
    "        # Application : [M, D] = [M, M] @ [M, D]\n",
    "        y_i = A @ x_i.T\n",
    "        y[i] = y_i  # [M, D]\n",
    "\n",
    "    return x, y, (W_q, W_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, D, P, M, H, c):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.H = H\n",
    "        self.c = c\n",
    "\n",
    "        # Paramètres pour chaque tête (query et key)\n",
    "        self.W_q = nn.Parameter(torch.randn(H, P, D))\n",
    "        self.W_k = nn.Parameter(torch.randn(H, P, D))\n",
    "\n",
    "    def forward(self, x):  # x: [N, D, M]\n",
    "        N = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        attn_sum = 0.0\n",
    "        for h in range(self.H):\n",
    "            # Projection pour la tête h : on obtient Zq et Zk de forme [N, P, M]\n",
    "            Zq = torch.einsum(\"pd,ndm->npm\", self.W_q[h], x) / (self.D**0.5)\n",
    "            Zk = torch.einsum(\"pd,ndm->npm\", self.W_k[h], x) / (self.D**0.5)\n",
    "            \n",
    "            # Transposition pour obtenir Q et K de forme [N, M, P]\n",
    "            Q = Zq.transpose(1, 2)  # [N, M, P]\n",
    "            K = Zk.transpose(1, 2)  # [N, M, P]\n",
    "            \n",
    "            # Calcul des scores d'attention : \"nip, njp -> nij\" donne [N, M, M]\n",
    "            scores = torch.einsum(\"nip, njp -> nij\", Q, K)\n",
    "            attn = F.softmax(scores, dim=-1)\n",
    "            attn_sum += attn\n",
    "\n",
    "        attn_mean = attn_sum / self.H  # [N, M, M]\n",
    "        # Ajout de la skip connection (même pour chaque batch)\n",
    "        skip = self.c * torch.eye(self.M, device=device).unsqueeze(0)  # [1, M, M]\n",
    "        combined = attn_mean + skip  # [N, M, M]\n",
    "        \n",
    "        # Appliquer la matrice d'attention aux tokens\n",
    "        # x est de forme [N, D, M] et on souhaite conserver la dimension D\n",
    "        output = torch.einsum(\"ndm, nmk -> ndk\", x, combined)\n",
    "        return output  # [N, D, M]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSMIMultiHead(nn.Module):\n",
    "    def __init__(self, D, P_list, M, H_list, c):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadAttentionLayer(D, P, M, H, c)\n",
    "            for P, H in zip(P_list, H_list)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(D, M)\n",
    "\n",
    "    def forward(self, x):  # x: [N, D, M]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # [N, D, M]\n",
    "        \n",
    "        x_transposed = x.transpose(1, 2)  # [N, M, D] — utilisé plus tard dans la multiplication\n",
    "\n",
    "        # Prédiction de la matrice d'attention\n",
    "        attention_logits = self.output_layer(x_transposed)  # [N, M, M]\n",
    "        attention_probs = torch.softmax(attention_logits, dim=-1)  # [N, M, M]\n",
    "\n",
    "        # Sortie : application de la matrice d’attention sur x\n",
    "        # attention_probs [N, M, M] × x [N, D, M]ᵀ → [N, M, D]\n",
    "        y = torch.bmm(attention_probs, x_transposed)  # [N, M, D]\n",
    "\n",
    "        return y  # chaque ligne = prédiction du token à chaque position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, n_epochs=100, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)  # [N_train, M, M]\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy entraînement\n",
    "        pred_labels = y_pred.argmax(dim=-1)     # [N_train, M]\n",
    "        true_labels = y_train.argmax(dim=-1)    # [N_train, M]\n",
    "        train_acc = (pred_labels == true_labels).float().mean()\n",
    "\n",
    "        # Mode évaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(x_test)  # [N_test, M, M]\n",
    "            test_loss = loss_fn(y_test_pred, y_test)\n",
    "            pred_test = y_test_pred.argmax(dim=-1)\n",
    "            true_test = y_test.argmax(dim=-1)\n",
    "            test_acc = (pred_test == true_test).float().mean()\n",
    "\n",
    "        # Affichage\n",
    "        if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "            print(f\"Epoch {epoch:03d} | \"\n",
    "                  f\"Train Loss: {loss.item():.4f} | Train Acc: {train_acc.item()*100:.2f}% | \"\n",
    "                  f\"Test Loss: {test_loss.item():.4f} | Test Acc: {test_acc.item()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train Loss: 1.4586 | Train Acc: 12.66% | Test Loss: 1.4646 | Test Acc: 12.76%\n",
      "Epoch 010 | Train Loss: 1.4250 | Train Acc: 12.75% | Test Loss: 1.4344 | Test Acc: 12.82%\n",
      "Epoch 020 | Train Loss: 1.3983 | Train Acc: 12.85% | Test Loss: 1.4105 | Test Acc: 12.80%\n",
      "Epoch 030 | Train Loss: 1.3782 | Train Acc: 12.88% | Test Loss: 1.3923 | Test Acc: 12.94%\n",
      "Epoch 040 | Train Loss: 1.3635 | Train Acc: 12.92% | Test Loss: 1.3791 | Test Acc: 13.08%\n",
      "Epoch 050 | Train Loss: 1.3529 | Train Acc: 12.97% | Test Loss: 1.3696 | Test Acc: 13.15%\n",
      "Epoch 060 | Train Loss: 1.3454 | Train Acc: 12.98% | Test Loss: 1.3629 | Test Acc: 13.17%\n",
      "Epoch 070 | Train Loss: 1.3401 | Train Acc: 12.96% | Test Loss: 1.3582 | Test Acc: 13.18%\n",
      "Epoch 080 | Train Loss: 1.3362 | Train Acc: 12.97% | Test Loss: 1.3549 | Test Acc: 13.28%\n",
      "Epoch 090 | Train Loss: 1.3333 | Train Acc: 12.97% | Test Loss: 1.3526 | Test Acc: 13.29%\n",
      "Epoch 100 | Train Loss: 1.3311 | Train Acc: 12.96% | Test Loss: 1.3510 | Test Acc: 13.30%\n",
      "Epoch 110 | Train Loss: 1.3294 | Train Acc: 12.96% | Test Loss: 1.3500 | Test Acc: 13.29%\n",
      "Epoch 120 | Train Loss: 1.3281 | Train Acc: 12.96% | Test Loss: 1.3494 | Test Acc: 13.30%\n",
      "Epoch 130 | Train Loss: 1.3270 | Train Acc: 12.98% | Test Loss: 1.3491 | Test Acc: 13.34%\n",
      "Epoch 140 | Train Loss: 1.3260 | Train Acc: 12.99% | Test Loss: 1.3490 | Test Acc: 13.38%\n",
      "Epoch 150 | Train Loss: 1.3251 | Train Acc: 12.98% | Test Loss: 1.3491 | Test Acc: 13.39%\n",
      "Epoch 160 | Train Loss: 1.3243 | Train Acc: 12.97% | Test Loss: 1.3494 | Test Acc: 13.37%\n",
      "Epoch 170 | Train Loss: 1.3235 | Train Acc: 12.97% | Test Loss: 1.3497 | Test Acc: 13.39%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m DeepSMIMultiHead(D, P_list\u001b[38;5;241m=\u001b[39m[P]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, M\u001b[38;5;241m=\u001b[39mM, H_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Entraînement\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, n_epochs, lr)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N_train, M, M]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_train)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m, in \u001b[0;36mDeepSMIMultiHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):  \u001b[38;5;66;03m# x: [N, D, M]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 12\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N, D, M]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x_transposed \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [N, M, D] — utilisé plus tard dans la multiplication\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Prédiction de la matrice d'attention\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m, in \u001b[0;36mMultiHeadAttentionLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m attn_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Projection pour la tête h : on obtient Zq et Zk de forme [N, P, M]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     Zq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpd,ndm->npm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_q\u001b[49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     22\u001b[0m     Zk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd,ndm->npm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_k[h], x) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Transposition pour obtenir Q et K de forme [N, M, P]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "D, M = 64, 10\n",
    "P = 8\n",
    "N = 10000\n",
    "\n",
    "# Dataset\n",
    "x, y, W_star = generate_synthetic_dataset(N, D, M, P)\n",
    "x_all, y_all = x.float(), y.float()\n",
    "# Supposons que x_all, y_all ont la forme torch.Tensor\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Modèle élève\n",
    "model = DeepSMIMultiHead(D, P_list=[P]*2, M=M, H_list=[4]*2, c=1.0)\n",
    "\n",
    "# Entraînement\n",
    "train_model(model, x_train, y_train, x_test, y_test, n_epochs=200, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.9146e-03, 8.0455e-01, 7.4404e-04, 3.6880e-03, 4.5673e-05, 1.3396e-02,\n",
      "         9.8182e-04, 3.0481e-04, 2.7697e-02, 1.4368e-01],\n",
      "        [8.8791e-04, 6.5693e-02, 9.0829e-01, 6.4671e-05, 3.2075e-03, 1.1098e-03,\n",
      "         3.4199e-03, 3.4873e-03, 6.7115e-03, 7.1247e-03],\n",
      "        [6.7212e-04, 7.5002e-05, 3.5989e-03, 8.2282e-03, 2.4692e-05, 1.1135e-03,\n",
      "         1.6930e-05, 2.3824e-05, 2.7449e-01, 7.1176e-01],\n",
      "        [1.3887e-03, 1.2862e-04, 1.1318e-04, 9.9506e-01, 1.2304e-06, 2.7359e-04,\n",
      "         2.8722e-03, 8.4135e-05, 7.1825e-05, 2.7311e-06],\n",
      "        [7.2928e-03, 3.3191e-04, 2.1810e-02, 3.6675e-01, 5.8519e-03, 5.3821e-03,\n",
      "         6.4585e-03, 5.8602e-01, 6.7511e-05, 3.8685e-05],\n",
      "        [1.4963e-03, 3.4123e-05, 7.0987e-01, 1.0078e-01, 1.4058e-03, 2.1199e-04,\n",
      "         1.7471e-03, 1.8445e-01, 4.3007e-06, 1.3284e-06],\n",
      "        [1.5859e-02, 1.4389e-03, 7.9806e-02, 1.9862e-01, 7.8504e-02, 8.6380e-03,\n",
      "         2.0008e-01, 8.1958e-02, 3.3271e-01, 2.3911e-03],\n",
      "        [1.1363e-01, 2.1498e-01, 1.0144e-02, 6.0624e-04, 7.3440e-02, 1.2869e-01,\n",
      "         2.4279e-01, 7.6861e-05, 2.3868e-02, 1.9177e-01],\n",
      "        [9.0526e-03, 1.7698e-03, 2.9075e-01, 9.7204e-02, 3.8845e-02, 6.8396e-04,\n",
      "         3.0332e-01, 1.8046e-01, 1.5888e-02, 6.2026e-02],\n",
      "        [1.8359e-03, 8.7996e-04, 9.5238e-01, 2.0126e-04, 1.1329e-02, 1.5202e-03,\n",
      "         5.2800e-04, 3.1237e-02, 1.0996e-06, 8.4479e-05]])\n"
     ]
    }
   ],
   "source": [
    "print(y_all[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
