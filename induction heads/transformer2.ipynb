{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(N, D, M, P, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # W* du prof : 2 couches concaténées\n",
    "    W_star = torch.randn(2 * P, D)\n",
    "\n",
    "    # Entrées x ∈ ℝ^{N, D, M}\n",
    "    x = torch.randn(N, D, M)\n",
    "\n",
    "    # Projections z ∈ ℝ^{N, 2P, M}\n",
    "    z = torch.einsum(\"pd,ndm->npm\", W_star, x) / D**0.5\n",
    "\n",
    "    y = torch.zeros(N, M, M)\n",
    "    for i in range(N):\n",
    "        z1 = z[i, :P].T  # [M, P]\n",
    "        z2 = z[i, P:].T  # [M, P]\n",
    "        A = z1 @ z2.T    # [M, M]\n",
    "        y[i] = torch.softmax(A, dim=1)\n",
    "\n",
    "    return x, y, W_star\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, D, P, M, H, c):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.P = P\n",
    "        self.M = M\n",
    "        self.H = H\n",
    "        self.c = c\n",
    "\n",
    "        # Paramètres pour chaque tête (query et key)\n",
    "        self.W_q = nn.Parameter(torch.randn(H, P, D))\n",
    "        self.W_k = nn.Parameter(torch.randn(H, P, D))\n",
    "\n",
    "    def forward(self, x):  # x: [N, D, M]\n",
    "        N = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        attn_sum = 0.0\n",
    "        for h in range(self.H):\n",
    "            # Projection de x via W_q et W_k pour la tête h.\n",
    "            # Zq, Zk: [N, P, M]\n",
    "            Zq = torch.einsum(\"pd,ndm->npm\", self.W_q[h], x) / (self.D**0.5)\n",
    "            Zk = torch.einsum(\"pd,ndm->npm\", self.W_k[h], x) / (self.D**0.5)\n",
    "            \n",
    "            # On transpose pour avoir Q et K de forme [N, M, P]\n",
    "            Q = Zq.transpose(1, 2)  # [N, M, P]\n",
    "            K = Zk.transpose(1, 2)  # [N, M, P]\n",
    "            \n",
    "            # Calcul des scores d'attention : somme sur la dimension P\n",
    "            # Equation : \"nip, njp -> nij\" donne un tenseur de forme [N, M, M]\n",
    "            scores = torch.einsum(\"nip, njp -> nij\", Q, K)\n",
    "            attn = F.softmax(scores, dim=-1)  # [N, M, M]\n",
    "            attn_sum += attn\n",
    "\n",
    "        # Moyenne sur les têtes\n",
    "        attn_mean = attn_sum / self.H  # [N, M, M]\n",
    "        # Ajout de la skip connection : on ajoute c * I pour chaque exemple\n",
    "        skip = self.c * torch.eye(self.M, device=device).unsqueeze(0)  # [1, M, M]\n",
    "        combined = attn_mean + skip  # [N, M, M]\n",
    "        \n",
    "        # Appliquer l'attention à x : \n",
    "        # x est de forme [N, D, M] et on veut obtenir un résultat de forme [N, D, M].\n",
    "        # On utilise l'einsum \"ndm, nmk -> ndk\" (ici k correspond à l'indice de token, de taille M)\n",
    "        output = torch.einsum(\"ndm, nmk -> ndk\", x, combined)\n",
    "        return output  # [N, D, M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSMIMultiHead(nn.Module):\n",
    "    def __init__(self, D, P_list, M, H_list, c):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadAttentionLayer(D, P, M, H, c)\n",
    "            for P, H in zip(P_list, H_list)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(D, M)  # prédit les scores d’attention\n",
    "\n",
    "    def forward(self, x):  # x: [N, D, M]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # [N, D, M]\n",
    "\n",
    "        # Transposer pour avoir [N, M, D]\n",
    "        x_t = x.transpose(1, 2)\n",
    "\n",
    "        # Prédire la matrice d’attention : [N, M, M]\n",
    "        logits = self.output_layer(x_t)  # [N, M, M]\n",
    "        attention = torch.softmax(logits, dim=-1)  # sur les lignes\n",
    "\n",
    "        return attention  # [N, M, M]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_attention(pred, target, eps=1e-8):\n",
    "    # pred, target ∈ [N, M, M]\n",
    "    pred = torch.clamp(pred, eps, 1. - eps)  # éviter log(0)\n",
    "    loss = - (target * torch.log(pred)).sum(dim=-1)  # [N, M]\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_attention_similarity(pred, target):\n",
    "    # pred, target ∈ [N, M, M]\n",
    "    pred = F.normalize(pred, p=2, dim=-1)   # normalisation ligne par ligne\n",
    "    target = F.normalize(target, p=2, dim=-1)\n",
    "    sim = (pred * target).sum(dim=-1)       # [N, M]\n",
    "    return sim.mean()                       # scalaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, n_epochs=100, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)  # [N, M, M]\n",
    "        train_loss = cross_entropy_attention(y_pred, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Cosine similarity d'entraînement\n",
    "        train_cos_sim = cosine_attention_similarity(y_pred, y_train)\n",
    "\n",
    "        # Évaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = model(x_test)\n",
    "            test_loss = cross_entropy_attention(y_test_pred, y_test)\n",
    "            test_cos_sim = cosine_attention_similarity(y_test_pred, y_test)\n",
    "\n",
    "        # Affichage\n",
    "        if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "            print(f\"Epoch {epoch:03d} | \"\n",
    "                  f\"Train CE: {train_loss.item():.4f} | Train CosSim: {train_cos_sim.item():.4f} | \"\n",
    "                  f\"Test CE: {test_loss.item():.4f} | Test CosSim: {test_cos_sim.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: torch.Size([800, 64, 10])\n",
      "y_train: torch.Size([800, 10, 10])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cosine_attention_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m DeepSMIMultiHead(D, P_list\u001b[38;5;241m=\u001b[39m[P]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, M\u001b[38;5;241m=\u001b[39mM, H_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Entraînement\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, n_epochs, lr)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Cosine similarity d'entraînement\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_cos_sim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_attention_similarity\u001b[49m(y_pred, y_train)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Évaluation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cosine_attention_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "D, M = 64, 10\n",
    "P = 8\n",
    "N = 1000\n",
    "\n",
    "# Génération du dataset synthétique\n",
    "\n",
    "x_all, y_all, W_star = generate_synthetic_dataset(N, D, M, P)\n",
    "x_all, y_all = x_all.float(), y_all.float()\n",
    "\n",
    "# Split train / test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "\n",
    "\n",
    "# Création du modèle élève : par exemple 2 couches avec 4 têtes chacune\n",
    "model = DeepSMIMultiHead(D, P_list=[P]*2, M=M, H_list=[4]*2, c=1.0)\n",
    "\n",
    "# Entraînement\n",
    "train_model(model, x_train, y_train, x_test, y_test, n_epochs=200, lr=1e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
